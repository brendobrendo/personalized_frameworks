# Personalized Framework for Computers

This document serves as a personalized framework to deepen my understanding of the computing domain. It covers essential concepts, models, cycles, and historical perspectives, focusing on the things that make computers excel at tasks humans find difficult.

---

## Helpful Terms to Keep Top of Mind
- **Harvard Architecture**: A computer architecture that uses separate storage for program instructions and data.
- **OSI Model**: A conceptual framework used to understand network interactions in seven layers.
- **Fetch-Decode-Execute Cycle**: The basic operational cycle of a computer where instructions are fetched, decoded, and executed.
- **Back of the Envelope Calculations**: Quick, approximate calculations used to estimate values in various computing contexts.
- **Computing History**: Key milestones and developments that have shaped modern computing.
- **Machine Efficiency**: Why computers excel at computational tasks that require high precision and large-scale processing.

---

## Harvard Architecture Framework

The Harvard Architecture is a computer architecture with physically separate storage for instructions (program) and data, unlike the Von Neumann architecture where both share the same memory. This separation allows for more efficient processing, especially in embedded systems.

---

## OSI Model

The **Open Systems Interconnection (OSI) model** is a conceptual framework used to understand and standardize how different networking protocols work together. The OSI model breaks down networking into seven layers:

1. **Physical Layer**: Transmission of raw data over physical media.
2. **Data Link Layer**: Error detection and correction in data transfer.
3. **Network Layer**: Routing of data across different networks.
4. **Transport Layer**: Reliable data transfer between devices.
5. **Session Layer**: Establishing, maintaining, and terminating sessions.
6. **Presentation Layer**: Data encoding and encryption.
7. **Application Layer**: Interaction with software applications and network services.

---

## Fetch-Decode-Execute Cycle

The **Fetch-Decode-Execute Cycle** is the basic cycle that all modern computers follow to execute instructions:

1. **Fetch**: The processor retrieves an instruction from memory.
2. **Decode**: The instruction is decoded to understand what operation needs to be performed.
3. **Execute**: The operation is performed using the appropriate resources (e.g., ALU for arithmetic operations).

---

## Back of the Envelope Calculations

These are quick estimations used to get a rough idea of a system’s computational capabilities or requirements. For example:
- Estimating how long it would take to process a dataset using a specific algorithm.
- Estimating power consumption based on a system’s components and expected load.

---

## History of Computing

Computing has a rich history, from early mechanical devices to the creation of digital computers. Key milestones include:
- **1930s**: The advent of the first electromechanical computers.
- **1940s**: The creation of ENIAC, one of the first fully electronic digital computers.
- **1960s**: The development of time-sharing systems and the rise of personal computing.

---

## Why Computers are Good at Things That Humans are Not

Computers excel in areas such as:
- **Speed and Precision**: Performing millions of calculations in seconds with no margin for error.
- **Repetitive Tasks**: Executing highly repetitive processes without fatigue or loss of performance.
- **Large-Scale Data Processing**: Managing vast amounts of data, analyzing trends, and making predictions faster than the human brain could comprehend.

---

Feel free to modify and expand upon each section based on your learning or project’s needs. This framework will act as a tool to deepen understanding and apply the concepts in practical scenarios.
